# Weekly Tasks

### 9/13/2023
1. clone the repo to you local machine learn -> updates your code to the develop branch
2. learn jupyter notebook in Python
3. install VSCode, conda (optional)
4. try YOLOV8: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb
5. try MediaPipe: https://mediapipe-studio.webapps.google.com/home (optional)

### 9/18/2023
1. learn Backpropagation Algorithm
2. understand the architecture of neural network
3. understand what is <strong>learnig rate, momentum, dropout, overfitting/underfitting</strong>, and why they are important
4. reference: http://www.cse.unsw.edu.au/~cs9417ml/MLP2/

### 9/25/2023
1. learn docker container
2. install docker to your local machine
3. start any container instance
4. understand what is docker image and container, and the difference between them
5. CLI cheat sheet: https://dockerlabs.collabnix.com/docker/cheatsheet/

### 10/02/2023
1. build docker image and start an instance as a container from dockerfile
2. build an frontend application (streamlit) in a container
3. build an backend application (pytorch) in a container
4. understand how docker-compose works
5. 


### 10/09/2023
1. create an app in the frontend
2. in this app, allow user upload a video file (.mp4 formate), filesize limit to 200 MB
3. save it to /storage, sametime, send the file_path to backend
4. in the backend, read the video file, and process it using YOLO/Mediapipe by human pose detection, save it to /storage, and send the filepath to frontend
5. in the frontend, display two videos side by side
6. optional task: looking for human pose classification model


### 10/16/2023
1. Continue work on the docker containerized application
2. expore human pose classification model
3. optional task: inference with LLM such as ChatGPT


### 10/23/2023
1. fork from the main branch, and try to make that branch work
2. focus on the human pose classification model (https://github.com/dronefreak/human-action-classification) or any other repo
3. create video datasets base on instructions


### 10/30/2023
1. expore the something2something & kinetic 400 datasets, looking for useful human action video clips
2. keep collection more useful videos from selfs
3. train a more robust model based on that


### 11/06/2023
1. expore the something2something & kinetic 400 datasets, looking for website allows you to download videos in specific action labels
2. Ming Chen go ask Dr. Lin see if we can buy some disk for storage videos
3. perform video action image data augmentation for better model training


### 11/13/2023
1. waiting for the Hard Drive to store videos data
2. downloading the kinetic 400 datasets
3. start playing model training using the 5% subset of kinetic 400 dataset before the HD coming


### 02/05/2024
1. Transformer, read paper (https://arxiv.org/abs/1706.03762), model design, the difference from deep learning model
2. GPT3, read the research paper (https://arxiv.org/abs/2005.14165)
3. review the action recognition model done by last semester


### 02/12/2024
1.  continue read the papers
2.  learn what are 1) positioning; 2) attention; 3) self-attention in transformer model
3.  read vision transformer: https://arxiv.org/abs/2010.11929


### 02/26/2024
1. Prepare 1 page PPT: 1) GPT-3 2) Vision Transformer model 3) OpenAI CLIP model 4) or any other LLM model you proposed
2. set up the connection to HPC system "redhawk.hpc.miamioh.edu" by watching the recordings
3. work on set up ask-anything repo (https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat) (optinal)
4. download all the needed models (optinal)


### 03/04/2024
1. Set up redhawk server, and load nvidia CUDA related modules
3. Work on set up ask-anything repo (https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat)
4. Download all the needed models 