{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import traceback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt at Running Model From Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m-pose.pt')\n",
    "results = model(source=0, show=True, conf=.3, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has a few problems. First there is no way to analyze individual frames and secondly, the video can't be stopped until you reset the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints and Angles Between Joints\n",
    "\n",
    "I want to get each joint from each individual frame and calculate the angles for each joint. Before I had no way of getting data on a single frame, however, if I use openCV to capture from my camera I can pass in the frames individually. After running the YOLOv8 model on a single frame it returns its results. From this you are able to get the 17 keypoints making up the pose. Each of these points represent the pixel coordinates of each join. The points are all stored in an array in the following order:\n",
    "\n",
    "0. nose\n",
    "1. left eye \n",
    "2. right eye\n",
    "3. left ear\n",
    "4. right ear\n",
    "5. left shoulder\n",
    "6. right shoulder\n",
    "7. left elbow\n",
    "8. right elbow\n",
    "9. left wrist\n",
    "10. right wrist\n",
    "11. left waist\n",
    "12. right waist\n",
    "13. left knee\n",
    "14. right knee\n",
    "15. left ankle\n",
    "16. right ankle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(point_A, point_B, point_C):\n",
    "    # Calculate vectors from B to A and B to C\n",
    "    vector_AB = point_A - point_B\n",
    "    vector_BC = point_C - point_B\n",
    "\n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = np.dot(vector_AB, vector_BC)\n",
    "\n",
    "    # Calculate the magnitudes (lengths) of the vectors\n",
    "    magnitude_AB = np.linalg.norm(vector_AB)\n",
    "    magnitude_BC = np.linalg.norm(vector_BC)\n",
    "\n",
    "    # Calculate the cosine of the angle θ using the dot product formula\n",
    "    cosine_theta = dot_product / (magnitude_AB * magnitude_BC)\n",
    "\n",
    "    # Calculate the angle θ in radians using the arccosine function\n",
    "    angle_radians = np.arccos(cosine_theta)\n",
    "\n",
    "    # Convert the angle from radians to degrees\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "    return (angle_degrees, angle_radians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating Angles on Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 335.6ms\n",
      "Speed: 2.0ms preprocess, 335.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 347.9ms\n",
      "Speed: 1.1ms preprocess, 347.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 318.2ms\n",
      "Speed: 1.0ms preprocess, 318.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 325.6ms\n",
      "Speed: 1.0ms preprocess, 325.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 310.2ms\n",
      "Speed: 1.0ms preprocess, 310.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 319.6ms\n",
      "Speed: 1.0ms preprocess, 319.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 315.1ms\n",
      "Speed: 1.0ms preprocess, 315.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 319.6ms\n",
      "Speed: 1.0ms preprocess, 319.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 316.6ms\n",
      "Speed: 1.0ms preprocess, 316.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 316.6ms\n",
      "Speed: 1.0ms preprocess, 316.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 327.6ms\n",
      "Speed: 1.0ms preprocess, 327.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 331.5ms\n",
      "Speed: 2.0ms preprocess, 331.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 356.7ms\n",
      "Speed: 1.0ms preprocess, 356.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 322.6ms\n",
      "Speed: 1.0ms preprocess, 322.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 314.5ms\n",
      "Speed: 1.0ms preprocess, 314.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Angle indices\n",
    "joint_angle_indices = {\n",
    "    \"left_elbow\": [5, 7, 9],\n",
    "    \"right_elbow\": [6, 8, 10],\n",
    "    \"left_shoulder\": [11, 5, 7],\n",
    "    \"right_shoulder\": [12, 6, 8],\n",
    "    \"left_waist\": [5, 11, 13],\n",
    "    \"right_waist\": [6, 12, 14],\n",
    "    \"left_knee\": [11, 13, 15],\n",
    "    \"right_knee\": [12, 14, 16]\n",
    "}\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8m-pose.pt')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLOv8 inference on the frame\n",
    "            results = model(source=frame, conf=.3)\n",
    "\n",
    "            # Visualize the pose results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "\n",
    "            # Get keypoints from results\n",
    "            pixel_keypoints = results[0].keypoints.xy[0]\n",
    "\n",
    "            # Calculate and annotate the angle of each joints\n",
    "            if (len(pixel_keypoints) != 0):\n",
    "                for joint_name in joint_angle_indices:\n",
    "                    indices = joint_angle_indices[joint_name]\n",
    "                    img_points = [np.array(pixel_keypoints[i]) for i in indices]\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        str(calc_angle(*img_points)[0]),\n",
    "                        tuple(map(int, img_points[1])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1\n",
    "                    )\n",
    "\n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "except Exception as err:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
