{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt at Running Model From Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m-pose.pt')\n",
    "results = model(source=0, show=True, conf=.3, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has a few problems. First there is no way to analyze individual frames and secondly, the video can't be stopped until you reset the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints and Angles Between Joints\n",
    "\n",
    "I want to get each joint from each individual frame and calculate the angles for each joint. Before I had no way of getting data on a single frame, however, if I use openCV to capture from my camera I can pass in the frames individually. After running the YOLOv8 model on a single frame it returns its results. From this you are able to get the 17 keypoints making up the pose. Each of these points represent the pixel coordinates of each join. The points are all stored in an array in the following order:\n",
    "\n",
    "0. nose\n",
    "1. left eye \n",
    "2. right eye\n",
    "3. left ear\n",
    "4. right ear\n",
    "5. left shoulder\n",
    "6. right shoulder\n",
    "7. left elbow\n",
    "8. right elbow\n",
    "9. left wrist\n",
    "10. right wrist\n",
    "11. left waist\n",
    "12. right waist\n",
    "13. left knee\n",
    "14. right knee\n",
    "15. left ankle\n",
    "16. right ankle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(point_A, point_B, point_C):\n",
    "    # Calculate vectors from B to A and B to C\n",
    "    vector_AB = point_A - point_B\n",
    "    vector_BC = point_C - point_B\n",
    "\n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = np.dot(vector_AB, vector_BC)\n",
    "\n",
    "    # Calculate the magnitudes (lengths) of the vectors\n",
    "    magnitude_AB = np.linalg.norm(vector_AB)\n",
    "    magnitude_BC = np.linalg.norm(vector_BC)\n",
    "\n",
    "    # Calculate the cosine of the angle θ using the dot product formula\n",
    "    cosine_theta = dot_product / (magnitude_AB * magnitude_BC)\n",
    "\n",
    "    # Calculate the angle θ in radians using the arccosine function\n",
    "    angle_radians = np.arccos(cosine_theta)\n",
    "\n",
    "    # Convert the angle from radians to degrees\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "    return (angle_degrees, angle_radians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating Angles on Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FPS variables\n",
    "frame_count = 0\n",
    "fps = -1\n",
    "start_time = time.time()\n",
    "\n",
    "# Angle indices\n",
    "joint_angle_indices = {\n",
    "    \"left_elbow\": [5, 7, 9],\n",
    "    \"right_elbow\": [6, 8, 10],\n",
    "    \"left_shoulder\": [11, 5, 7],\n",
    "    \"right_shoulder\": [12, 6, 8],\n",
    "    \"left_waist\": [5, 11, 13],\n",
    "    \"right_waist\": [6, 12, 14],\n",
    "    \"left_knee\": [11, 13, 15],\n",
    "    \"right_knee\": [12, 14, 16]\n",
    "}\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "\n",
    "            # Calculate and display FPS\n",
    "            frame_count += 1\n",
    "            if frame_count >= 10:  # Calculate FPS every 10 frames\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                fps = frame_count / elapsed_time\n",
    "                frame_count = 0\n",
    "                start_time = end_time\n",
    "\n",
    "            # Run YOLOv8 inference on the frame\n",
    "            results = model(source=frame, conf=.3)\n",
    "\n",
    "            # Get keypoints from results\n",
    "            pixel_keypoints = results[0].keypoints.xy[0]\n",
    "\n",
    "            # Calculate and annotate the angle of each joints\n",
    "            if (len(pixel_keypoints) != 0):\n",
    "                # Visualize the pose results on the frame\n",
    "                annotated_frame = results[0].plot()\n",
    "\n",
    "                for joint_name in joint_angle_indices:\n",
    "                    indices = joint_angle_indices[joint_name]\n",
    "                    img_points = [np.array(pixel_keypoints[i]) for i in indices]\n",
    "                    cv2.putText(\n",
    "                        annotated_frame,\n",
    "                        str(calc_angle(*img_points)[0]),\n",
    "                        tuple(map(int, img_points[1])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1\n",
    "                    )\n",
    "\n",
    "                # Display FPS\n",
    "                cv2.putText(annotated_frame, 'FPS: ' + str(fps), (0,25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1)\n",
    "\n",
    "                # Display the annotated frame\n",
    "                cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "            else:\n",
    "                # Display FPS\n",
    "                cv2.putText(frame, 'FPS: ' + str(fps), (0,0), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "                # Show frame\n",
    "                cv2.imshow(\"\", frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "except Exception as err:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaPipe\n",
    "\n",
    "[Download Models](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models)\n",
    "<br>\n",
    "[Docs](https://developers.google.com/mediapipe/api/solutions/python/mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import traceback\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FPS variables\n",
    "frame_count = 0\n",
    "fps = -1\n",
    "start_time = time.time()\n",
    "\n",
    "# Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker_full.task')\n",
    "options = vision.PoseLandmarkerOptions(base_options=base_options)\n",
    "model = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Calculate and display FPS\n",
    "            frame_count += 1\n",
    "            if frame_count >= 10:  # Calculate FPS every 10 frames\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                fps = frame_count / elapsed_time\n",
    "                frame_count = 0\n",
    "                start_time = end_time\n",
    "\n",
    "            # Convert the OpenCV frame to RGB format (mediapipe expects RGB)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.asarray(rgb_frame))\n",
    "\n",
    "            # Detect pose landmarks from the input image.\n",
    "            results = model.detect(image)\n",
    "\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                # Draw the pose landmarks on the frame\n",
    "                annotated_image = draw_landmarks_on_image(image.numpy_view(), results)\n",
    "\n",
    "                # Display FPS\n",
    "                cv2.putText(annotated_image, 'FPS: ' + str(fps), (0,25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1)\n",
    "\n",
    "                cv2.imshow('', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "            else:\n",
    "                # Display FPS\n",
    "                cv2.putText(frame, 'FPS: ' + str(fps), (0,25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1)\n",
    "\n",
    "                cv2.imshow('', frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "except Exception as err:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FPS variables\n",
    "frame_count = 0\n",
    "fps = -1\n",
    "start_time = time.time()\n",
    "\n",
    "# Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "model = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Calculate and display FPS\n",
    "            frame_count += 1\n",
    "            if frame_count >= 10:  # Calculate FPS every 10 frames\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                fps = frame_count / elapsed_time\n",
    "                frame_count = 0\n",
    "                start_time = end_time\n",
    "\n",
    "            # Convert the OpenCV frame to RGB format (mediapipe expects RGB)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.asarray(rgb_frame))\n",
    "\n",
    "            # Detect pose landmarks from the input image.\n",
    "            results = model.detect(image)\n",
    "\n",
    "            if results.hand_landmarks:\n",
    "                # Draw the pose landmarks on the frame\n",
    "                annotated_image = draw_landmarks_on_image(image.numpy_view(), results)\n",
    "\n",
    "                # Display FPS\n",
    "                cv2.putText(annotated_image, 'FPS: ' + str(fps), (0,25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1)\n",
    "\n",
    "                cv2.imshow('', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "            else:\n",
    "                # Display FPS\n",
    "                cv2.putText(frame, 'FPS: ' + str(fps), (0,25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 0), 1)\n",
    "\n",
    "                cv2.imshow('', frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "except Exception as err:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
