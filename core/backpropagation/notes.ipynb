{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Notes\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=aVId8KMsdUU)\n",
    "\n",
    "* Backpropagation:\n",
    "  * Backpropagation is the algorithm used to calculate gradients of the loss function with respect to the weights of the neural network. It's used to update the weights in the training process.\n",
    "\n",
    "* Activation Function:\n",
    "  * Activation functions introduce non-linearity into neural networks. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh. They determine the output of a neuron given its input.\n",
    "\n",
    "* Loss Function (Cost Function):\n",
    "  * The loss function measures the error between the predicted values and the actual target values. It quantifies how well or poorly the neural network is performing.\n",
    "\n",
    "* Gradient Descent:\n",
    "  * Gradient descent is an optimization algorithm used to update the weights of a neural network during training. It aims to minimize the loss function by iteratively adjusting the weights in the direction of the steepest descent (negative gradient).\n",
    "\n",
    "* Learning Rate:\n",
    "  * The learning rate is a hyperparameter that determines the size of steps taken during the optimization process when updating the weights of a neural network.\n",
    "  * In the context of backpropagation, after each forward and backward pass through the network (one training iteration), the weights of the network are updated to minimize the loss function. The learning rate controls the size of these weight updates.\n",
    "  * A high learning rate can lead to quick convergence but may result in overshooting the optimal solution or getting stuck in local minima. On the other hand, a very low learning rate may make the training process extremely slow or even prevent the model from converging.\n",
    "  * Typically, learning rates are set based on experimentation and cross-validation. Common values include 0.1, 0.01, 0.001, and so on.\n",
    "\n",
    "* Momentum:\n",
    "  * Momentum is another hyperparameter used during the training of neural networks to help accelerate the convergence process.\n",
    "  * In traditional gradient descent, weight updates are solely based on the gradient of the current mini-batch of training data. Momentum introduces a moving average of past gradients into the weight update equation.\n",
    "  * This moving average (the momentum term) allows the optimization process to build up momentum in directions where gradients have been consistently pointing over time. This helps the optimization process overcome oscillations and converge faster.\n",
    "  * It acts like a ball rolling down a hill, gaining speed as it moves in a consistent direction.\n",
    "  * Common values for the momentum term are 0.9 or 0.99, but it can also be tuned based on the specific problem and dataset.\n",
    "\n",
    "* Dropout:\n",
    "  * Dropout is a regularization technique used to prevent overfitting in neural networks, especially deep neural networks.\n",
    "  * During training, dropout randomly sets a fraction of neurons' activations to zero at each forward pass. This means that during training, some neurons are \"dropped out\" or temporarily ignored.\n",
    "  * The dropout rate is a hyperparameter that controls the probability of a neuron being dropped out. Common dropout rates range from 0.2 to 0.5.\n",
    "  * Dropout introduces a form of redundancy into the network, as different subsets of neurons are active during each forward pass. This prevents the network from relying too heavily on any particular set of neurons and helps it generalize better to unseen data.\n",
    "  * It's important to note that dropout is only applied during training. During inference (when making predictions), dropout is typically turned off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
